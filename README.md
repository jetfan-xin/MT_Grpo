### Ltgpu3 GRPO-QE Experiment

### ç›®æ ‡

å°†word-level QEä½œä¸ºå¥–åŠ±æŒ‡æ ‡ä¹‹ä¸€ï¼ŒåŠ å…¥åˆ°å½“å‰çš„æœºå™¨ç¿»è¯‘è´¨é‡ä¼˜åŒ–ä»»åŠ¡ï¼ˆåŒ…æ‹¬ä¸­è‹±åŒè¯­äº’è¯‘ï¼‰ã€‚



å½“å‰æ€è·¯æ˜¯æŠŠMQM sentence-level QEä½œä¸ºè®­ç»ƒä¸­çš„å¥–åŠ±æŒ‡æ ‡ï¼Œæ›¿æ¢æ‰åŸæœ‰çš„comet+bleuã€‚è¿™ä¸ªè¯„åˆ†ç†è®ºä¸Šæ˜¯ç”¨error spansçš„ç¿»è¯‘é”™è¯¯ä¸¥é‡ç¨‹åº¦åŠ æƒè®¡ç®—å¾—åˆ°ï¼Œé—´æ¥èå…¥word-levelçš„ç‰¹ç‚¹ã€‚

å¯ä»¥ç”¨å„ç§ç¬¦åˆæ¡ä»¶çš„QEã€‚

å®ç°äº†ä½¿ç”¨2022å¹´cometkiwiè®ºæ–‡ä¸­çš„sentence-level MQMçš„ä»£ç ï¼Œè®­ç»ƒäº¤ç”±Jinghengã€‚

cometkiwiçš„sentence-level MQMä¸­æ–‡åˆ†è¯æ•ˆæœä¸å¥½ï¼Œæ•´å¥è¯„åˆ†èƒ½ç”¨ï¼Œä½†æ— æ³•å›è¿‡å¤´æ‰¾åˆ°å¯¹åº”çš„é”™è¯¯ç‰‡æ®µã€‚

æ¢ç´¢å…¶ä»–å¯ç”¨çš„æŒ‡æ ‡ï¼ŒåŒ…æ‹¬Cometkiwiï¼ŒxCOMETï¼ŒInstructscoreï¼ŒGEMBA-MQMã€‚

å‘ç°**xCOMET**æ—¢èƒ½ç»™å‡ºåˆç†çš„error spansï¼Œä¹Ÿèƒ½é€‚é…ä¸­è‹±åŒè¯­äº’è¯‘ï¼Œå¹¶ä¸”sentence-level MQMæ˜¯å®Œå…¨åŸºäºerror spansè®¡ç®—å¾—åˆ°çš„ã€‚é€‚åˆå½“å‰ä»»åŠ¡ã€‚



### ä¿®æ”¹æ­¥éª¤ï¼š

è¦åœ¨åŒä¸€ä¸ª verl-qe ç¯å¢ƒé‡Œâ€œåŒæ—¶ç”¨â€ï¼š

- åºåˆ—çº§ï¼ˆæ–°ç‰ˆï¼Œunified_metricï¼›wmt23 ckptï¼‰
- è¯çº§ï¼ˆæ—§ç‰ˆ legacyï¼Œunite_metric_multi_taskï¼›WMT22 word-level ckptï¼‰

#### 2025.9.6

0. QEmodelä¸ç›¸å…³çš„ï¼š

   - ä¿®æ”¹ comet_reward_batch.py -> åˆ†æˆä¸¤ç‰ˆ:

     - comet_reward_batch_wo_ray.py: ä¸ä½¿ç”¨ Rayï¼Œä¿®å¤è®¾å¤‡é€‰æ‹© (å¦‚device = 'cuda' if torch.cuda.is_available() else 'cpu')

     - comet_reward_batch_with_ray.py: ä½¿ç”¨ Ray actorï¼Œç»™ COMET å•ç‹¬åˆ†é…ä¸€å¼  GPUï¼Œæ­£ç¡®è°ƒç”¨ GPUï¼ˆä½†æ˜¯å½“æ‰€æœ‰å¡éƒ½ç»™GRPOç”¨äºè®­ç»ƒæ—¶ï¼Œå¯åŠ¨comet computationåå¡ä½ï¼‰

   - æ›´æ–°äº† custom_grpo_fast.sh è„šæœ¬

1. pip clone å¯¹åº” legacyåæˆ‘åšäº† pip install -e .ï¼Œä½†é™†é™†ç»­ç»­å‡ºç°åº“ç‰ˆæœ¬ä¾èµ–é—®é¢˜ï¼Œä¸»è¦ä¿®æ”¹åŒ…æ‹¬ï¼šå¸è½½torchvisionï¼Œæ›´æ”¹tranformerç‰ˆæœ¬ï¼Œæ›´æ”¹torch=2.6.0ï¼Œvllm=0.8.4ï¼Œè§£å†³ç‰ˆæœ¬å†²çªã€‚unbabel-comet==2.2.6

2. åœ¨ä½¿ç”¨wmt22-comet-legacy çš„cometåº“ï¼ˆç‰ˆæœ¬è¾ƒæ—§ï¼‰æ—¶å‘ç°ï¼Œç‰ˆæœ¬è¾ƒæ–°çš„cometkiwi sequence levelçš„ wmt23çš„ckptæ— æ³•ä½¿ç”¨ï¼Œå› ä¸ºå®ƒæŒ‡å®šå‚æ•°class_identifier: unified_metricï¼Œè€Œè¿™ä¸ªå‚æ•°åœ¨legacyçš„æ—§ç‰ˆæœ¬cometåº“ä¸­ä¸å­˜åœ¨ï¼Œä¼šæŠ¥é”™ã€‚

   æœ€ç»ˆè®¡åˆ’é‡‡ç”¨çš„è§£å†³æ–¹å¼æ˜¯ï¼š**å¹¶å­˜ä¸¤å¥—åŒ…ï¼Œä½†ç”¨ä¸åŒçš„æ¨¡å—åå¯¼å…¥**ã€‚ä¹Ÿå°±æ˜¯ï¼šè®©æ–°ç‰ˆç»§ç»­å« cometï¼Œä¿æŒä½ ç°æœ‰çš„ sequence-levelï¼ˆwmt23/24 unified_metricï¼‰è·¯å¾„ä¸å˜ã€‚æŠŠ legacy ä½œä¸ºâ€œæºç ç›®å½•â€ç”¨åˆ«åæ¯”å¦‚ comet_legacy åŠ¨æ€å¯¼å…¥ï¼ˆä¸å»è¦†ç›– site-packages é‡Œçš„æ–°ç‰ˆ cometï¼‰ï¼Œå¦èµ·ä¸€ä¸ª word-level legacy æ¨¡å‹å®ä¾‹ã€‚è¿™æ ·å°±å¯ä»¥åœ¨ predict æ—¶å¹¶è¡Œ/é¡ºåºè°ƒç”¨ä¸¤è·¯ï¼Œç„¶åæŠŠè¯çº§ç»“æœåŠ åˆ°è¿”å›çš„ extra_infosï¼Œæˆ–ç»„åˆåˆ°å¥–åŠ±é‡Œï¼ˆä¾‹å¦‚çº¿æ€§åŠ æƒã€é˜ˆå€¼è£å‰ªç­‰ï¼‰ã€‚

3. ç°åœ¨èƒ½å¤Ÿè·‘é€šword level cometliwiæŒ‡æ ‡è®¡ç®—ä¾‹å­ï¼ˆlegacyè·¯å¾„ä¸‹çš„test.pyï¼‰ã€‚

   - åœ¨ä¾‹å­è¾“å‡ºä¸­ï¼Œword level cometliwi predictçš„scoreä¸æ˜¯cometï¼Œæ˜¯å¥çº§ QE é¢„æµ‹å€¼ï¼ˆæ‹Ÿåˆ DA/MQM/HTER çš„å›å½’è¾“å‡ºï¼‰

4. åœ¨ç°æœ‰ comet_reward_batch.py é‡Œæ€ä¹ˆåŒæ—¶ç”¨ sequence-level + word-level

   1. æ”¹ç”¨**åˆ«ååŠ è½½**ï¼ˆä»…åœ¨éœ€è¦æ—¶æŠŠ legacy å½“ä½œä¸€ä¸ªâ€œå‘½åæ¨¡å—â€æ³¨å…¥ï¼‰ã€‚

   2. æŠŠ legacy åŒ…é‡Œçš„æ‰€æœ‰cometç»å¯¹å¯¼å…¥æ”¹æˆç›¸å¯¹å¯¼å…¥ã€‚å¦‚ï¼š

      ```python
      # from comet.models import available_metrics
      from .models import available_metrics
      ```

      ä½¿ç”¨æ‰¹é‡æŸ¥æ‰¾ä»£ç ï¼š

      ```shell
      grep -R "from comet\." -n ~/MT_Grpo_qe/wmt22-comet-legacy/comet || true
      # å¦‚æœæœ‰å…¶å®ƒå‘½ä¸­ï¼Œå†æŒ‰éœ€æŠŠ `from comet.xxx` æ”¹æˆ `from .xxx`
      ```

      æŠŠå±‚çº§ä¿®å¯¹ï¼éœ€è¦æŠŠæ·±å±‚æ¨¡å—çš„ç›¸å¯¹å±‚çº§æ”¹æ­£ç¡®ã€‚å¸¸è§çš„ä¿®æ³•æ˜¯ï¼š

      - é¡¶å±‚ comet/*.py é‡Œï¼š

        from comet.xxx import ... â†’ from .xxx import ...


      - å­ç›®å½• comet/models/*.py é‡Œï¼š
    
        from comet.models.base import ... â†’ from .base import ...
    
        from comet.models.utils import ... â†’ from .utils import ...


      - å­å­ç›®å½• comet/models/regression/*.pyã€comet/models/ranking/*.py é‡Œï¼š
    
        from comet.models.base import ... â†’ from ..base import ...
    
        from comet.models.utils import ... â†’ from ..utils import ...

      3.  **comet_reward_batch.pyä¸­è½½å…¥QE checkpoint**ã€‚ä¿®æ”¹custom_reward_batch.pyï¼Œåœ¨ comet_reward_batch.py é‡Œï¼Œç”¨â€œä¸´æ—¶åˆ«ååŠ è½½å™¨â€æŠŠ legacy çš„cometåŒ…åŠ è½½ä¸º comet_legacyã€‚åŠ è½½å®Œæˆåç«‹åˆ»æ¢å¤ï¼Œæ‰€ä»¥ä¸ä¼šæ±¡æŸ“åç»­å¯¹æ–°ç‰ˆ comet çš„ä½¿ç”¨ã€‚

      4.  æ¥ä¸‹æ¥éœ€è¦æŠŠword-level cometï¼ˆä»¥ä¸‹ç®€ç§°QEï¼‰rewardè®¡ç®—æ•´åˆåˆ°comet_reward_batch.py
          TBD

### 2025.9.16

1. QEå·²æ•´åˆåˆ°comet_reward_batch_qe.pyã€‚å› ä¸ºè®­ç»ƒå’ŒéªŒè¯å…±ç”¨ä¸€å¥—æŒ‡æ ‡è®¡ç®—å‡½æ•°ï¼Œå³comet_reward_batch_qe.pyä¸­çš„compute_score functionï¼Œå¯¹è¯¥å‡½æ•°ç›¸å…³çš„å‡½æ•°å¦‚compute_score_batchã€ä»£ç æ–‡ä»¶å¦‚ray_trainerç­‰æ·»åŠ compute_val_rewardåˆ¤æ–­å‚æ•°ï¼Œæ ¹æ®è®­ç»ƒ/éªŒè¯æƒ…æ™¯é€‰æ‹©ç›¸åº”çš„æŒ‡æ ‡ã€‚è®­ç»ƒä½¿ç”¨format+QEï¼›éªŒè¯ä½¿ç”¨format+sequence level cometkiwi+bleuã€‚

2. ä¸­æ–‡æ—¥æ–‡åˆ†è¯é—®é¢˜ï¼š
   - tagè®¡ç®—æ–¹å¼ï¼šé¦–å…ˆå°†ä¸€ä¸ªå¥å­ç”¨transformeråˆ†è¯ä¸ºtokensï¼ˆä»£ç ä¸­å˜é‡ä¸ºsubwordsï¼‰ï¼Œå¹¶è¯†åˆ«å“ªå‡ ä¸ªtokensæ‹¼æˆä¸€ä¸ªè¯ï¼ˆwordï¼‰ï¼Œå¯¹æ¯ä¸ªè¯é¦–ä¸ªtokenå‰æ–¹åŠ "â–"ã€‚åœ¨è¯çº§è®¡ç®—tagï¼Œä¸€ä¸ªè¯ä¸€ä¸ªtagã€‚
   - ä¸ºä»€ä¹ˆä¸­æ—¥æ–‡å¥å­åªæœ‰ä¸€ä¸ªtagï¼štransformerç”¨ç©ºæ ¼åšåˆ†ç•Œè¯†åˆ«è¯èŒƒå›´ï¼Œç©ºæ ¼åçš„ç¬¬ä¸€ä¸ªtokenä¼šè¢«è¯†åˆ«ä¸ºè¯å¤´ï¼ŒåŠ "â–"ã€‚è€Œä¸­æ—¥æ–‡æ²¡æœ‰ç©ºæ ¼ï¼Œä¹Ÿå°±ä¸ä¼šåŠ "â–"ï¼Œè¿™å¯¼è‡´åç»­åœ¨è¯çº§è®¡ç®—æ—¶ï¼Œä¼šæŠŠæ‰€æœ‰tokensè¯†åˆ«ä¸ºä¸€ä¸ªè¯ï¼Œåªå¾—åˆ°ä¸€ä¸ªtagsã€‚
   - ä¿®å¤æ–¹å¼ï¼šå¾ˆç®€å•ï¼Œåœ¨æ–‡æœ¬å¥å­è½¬subwords listçš„å‡½æ•°è¿è¡Œåï¼Œé€šè¿‡jieba/fugashiåˆ†è¯ï¼Œå‚ç…§åˆ†è¯ç»“æœç»™subwords listä¸­tokensåŠ "â–"ã€‚ï¼ˆwmt22-comet-legacy/comet/encoders/bert.pyä¸­ï¼‰
   - ä½†æ˜¯ä¸èƒ½è¿™æ ·åšï¼šå› ä¸ºè¯¥æŒ‡æ ‡è®­ç»ƒæ—¶ä½¿ç”¨åŸæœ¬çš„åˆ†tokenåˆ†è¯æ–¹å¼ï¼Œå¦‚æœæˆ‘åœ¨ä½¿ç”¨checkpoint predictæŒ‡æ ‡å€¼æ—¶æŠŠåŸæœ¬çš„æ–¹å¼æ›´æ”¹ï¼Œåˆ™å’Œcheckpointç®—æ³•çš„é€»è¾‘ä¸åŒï¼Œä¼šé€ æˆåå·®ã€‚æœ€ç»ˆæ²¡æœ‰å®ç°åˆ†è¯ã€‚



### 2025.10.7

ä»å‡ ä¸ªè®ºæ–‡é‡Œï¼Œcometkiwi, xcomet, instructscore, gemba mqmæˆ–è€…åˆ«çš„ä»€ä¹ˆllm baseçš„mqmæ–¹æ³•ï¼Œçœ‹çœ‹ä»–ä»¬æ€ä¹ˆç®—çš„error spançš„å‡†ç¡®ç‡çš„è¿˜æ˜¯ä»€ä¹ˆæŒ‡æ ‡ï¼Œç„¶åæµ‹ä¸€ä¸‹ä¸­åˆ°è‹±å’Œè‹±åˆ°ä¸­ã€‚

#### CometKiwi: 

MQMå’Œerror spansä½¿ç”¨ä¸åŒçš„æ¨¡å‹åˆ†åˆ«é¢„æµ‹ï¼ŒäºŒè€…æ²¡æœ‰å…³è”ã€‚

##### Corpora:

Critical: Fine-tuning: zh-en (75k+ samples) & en-zh (500 samples)

1. MQM: a sentence-level QE

   - Pretraining: WMT 2017â€“2019 Metrics Shared Task çš„ DA æ•°æ®

   - Fine-tuning: (not references) MQM annotations from WMT 2020 and 2021

2. Error span = a word-level QE task = predict OK/BAD labels for each word (combined by subwords).

   æ¨¡å‹åœ¨è®­ç»ƒæ—¶åˆ†ä¸ºä¸¤ä¸ªä¸»è¦æ¥æºç±»å‹ï¼ˆå³ä¸¤ç±»è¯­æ–™ï¼‰

   1. Post-edit originated LPs: äººç±»åœ¨æœºå™¨ç¿»è¯‘åŸºç¡€ä¸Šåšä¿®æ”¹åçš„è¯­æ–™

      Pretraining:

      - WMT 2017â€“2019 Metrics Shared Task çš„ DA æ•°æ®; 
      - QT21 and 
      - APEQuest that include both word-level labels and sentence (HTER) scores

   2. MQM originated LPs: MQMæ ‡æ³¨æ•°æ®

      Fine-tuning: (not references)

      - MQM annotations from WMT 2020 and 2021

      - Improvement: concatenated DA and MQM datasets together for a single fine-tuning.

##### Result Table:

Word-level QE on Post-edit originated LPs:  No zh-en & en-zh

MQM & word-level QE on MQM originated LPs: Only zh-en, no en-zh



#### ğŸŸ¢ xCOMET: 

ä½¿ç”¨ä¸€ä¸ªç»Ÿä¸€æ¨¡å‹é¢„æµ‹sentence-level MQMå’Œerror spans (combined by subwords)ã€‚MQMç›´æ¥ç”±error spansè®¡ç®—å‡ºæ¥ï¼ˆâ€œlearn-to-detectâ€ errors â†’ infer MQMï¼‰ã€‚xCOMET çš„ error span æ¨¡å‹åªé¢„æµ‹é”™è¯¯çš„ä¸¥é‡ç¨‹åº¦ï¼ˆminorã€majorã€criticalã€OKï¼‰ï¼Œä¸é¢„æµ‹é”™è¯¯çš„ç±»å‹ï¼ˆ fluencyã€accuracy ç­‰ï¼‰ã€‚

##### Corpora: (WMT22ä¸­æœ‰en-zh & zh-en MQM)

1. MQM annotations sourced from WMT from 2020 to 2022.
2. IndicMT
3. DEMETR

including source, MT, and references

##### Results Table:

Only zh-en, no en-zh.



#### Instructscore:

##### Corporaï¼š(no En-Zh)

https://github.com/xu1998hz/InstructScore_SEScore3/tree/main/data

ä¸»è¦ä¾èµ– GPT-4 è‡ªåŠ¨ç”Ÿæˆçš„ MQM é£æ ¼ä¼ªè¯­æ–™ è¿›è¡Œå¤§è§„æ¨¡è®­ç»ƒï¼Œ
å†åˆ©ç”¨ WMT22 MQM (Zhâ†’En, Enâ†’De) æ•°æ®åšå°è§„æ¨¡éªŒè¯ä¸å¯¹é½

1. Sentence-level MQM prediction
   - GPT-4 åˆæˆè¯„åˆ†è¯­æ–™
   - DEMETR
   - WMT22 MQM (Zhâ†’En, Enâ†’De)

2. Error Span predictionï¼ˆé”™è¯¯ç±»å‹ã€ä½ç½®ã€ä¸¥é‡åº¦ã€è§£é‡Šå­—æ®µçš„å‡†ç¡®ç‡ä¸å¬å›ç‡ï¼‰
   - GPT-4 åˆæˆé”™è¯¯æ ‡æ³¨è¯­æ–™
   - DEMETR 
   - MLQE-PE 
   - WMT22 MQM

| **Corpora**                   | **æè¿°**                                                     |
| ----------------------------- | ------------------------------------------------------------ |
| **GPT-4 åˆæˆè¯­æ–™**            | 10k å¥å­ Ã— 100 åŸŸï¼ˆnewsã€medicalã€legalã€social ç­‰ï¼‰ï¼Œè¦†ç›–å¤šç§ NLG ä»»åŠ¡ã€‚ |
| **Synthetic Error Injection** | GPT-4 åœ¨åŸæ–‡ä¸Šæ³¨å…¥ 1â€“5 ä¸ªé”™è¯¯ï¼ˆerror typeã€severityã€ä½ç½®ã€è§£é‡Šï¼‰ï¼Œæ¨¡æ‹Ÿ MQM å¼æ ‡æ³¨ã€‚ |
| **Error Type é›†**             | æ¥è‡ª MQM æ¡†æ¶ï¼ˆFreitag et al., 2021ï¼‰ï¼šAddition, Omission, Mistranslation, Grammar, Style, Terminology ç­‰ã€‚ |
| **Fine-tuning æ•°æ®è§„æ¨¡**      | æ¯ç§ä»»åŠ¡ç”Ÿæˆçº¦ 10k pseudo pairsï¼ˆreference + candidate + diagnostic reportï¼‰ã€‚ |
| **Refinement é˜¶æ®µæ•°æ®**       | ä» WMT20 ç³»ç»Ÿè¾“å‡ºï¼ˆ2,000 Zhâ†’En æ ·æœ¬ï¼‰é‡‡æ ·ï¼ŒGPT-4 è‡ªåŠ¨è¯„ä»·æ¨¡å‹è¾“å‡ºçš„ failure modes å¹¶è¿›ä¸€æ­¥å¾®è°ƒã€‚ |



#### GEMBA-MQM

 GEMBA-MQM å¹¶æ²¡æœ‰ä½¿ç”¨ä»»ä½•â€œè®­ç»ƒæ•°æ®â€è¿›è¡Œå‚æ•°å­¦ä¹ ã€‚åŸºäº GPT-4 é€šè¿‡ promptï¼ˆfixed three-shot prompting techniqueï¼‰ç›´æ¥æ‰§è¡Œ MQM é£æ ¼é”™è¯¯æ£€æµ‹ã€‚ç›¸å½“äºç›´æ¥ç”¨OpenAI GPT4è¿›è¡Œæ£€æµ‹ï¼Œä¸æ˜¯ä¸€ä¸ªå¯ä¸‹è½½ã€å¯è®­ç»ƒçš„æ¨¡å‹ï¼Œ

å’Œæˆ‘ä»¬éœ€è¦å¿«é€Ÿæµ‹è¯„ç”¨äºè®­ç»ƒç¯èŠ‚çš„ä»»åŠ¡ä¸é€‚é…ã€‚



#### WMT22-24

è¿‡å»ä¸‰å¹´å®˜æ–¹æµ‹è¯„æ•°æ®ä¸­æ²¡æœ‰En-Zhã€‚ä½†æ˜¯å¯ä»¥å‚è€ƒMQM evaluationæ’åï¼Œçœ‹æ•ˆæœæ›´å¥½çš„æŒ‡æ ‡æ˜¯å¦åœ¨en-zhæ•°æ®é›†ä¸Šè®­ç»ƒï¼Œæœ‰en-zhç¿»è¯‘æµ‹è¯„èƒ½åŠ›ã€‚

22: Englishâ†’German,  Englishâ†’Russian, Chineseâ†’English

23: Englishâ†’German, Chineseâ†’English, Hebrewâ†’English

24: Englishâ†’German, Japaneseâ†’Chinese, Englishâ†’Spanish





### 2025.10.8

1. å°†æ•°æ®ã€æ¨¡å‹ã€checkpointsç»“æœã€ç¯å¢ƒï¼ˆcopyï¼‰è¿ç§»åˆ°å…¬å…±ç©ºé—´/mnt/data1/users/4xin
2. åœ¨ï½./bashrcä¸­è®¾ç½®äº† HF_HOME / HF_HUB_CACHEï¼š
   - HF_HOMEï¼šHugging Face çš„â€œæ€»å®¶ç›®å½•â€ã€‚è®¾ç½®åï¼ŒHub æ¨¡å‹ç¼“å­˜ã€datasets ç¼“å­˜ç­‰éƒ½ä¼šé»˜è®¤æ”¾åˆ°è¿™ä¸ªæ ¹ç›®å½•ä¸‹ï¼ˆå¦‚ HF_HOME/hubã€HF_HOME/datasetsã€HF_HOME/transformersï¼‰ã€‚
   - HF_HUB_CACHEï¼šä¸“é—¨æŒ‡å®šâ€œHub æ¨¡å‹ç¼“å­˜ç›®å½•â€ã€‚ä¼˜å…ˆçº§é«˜äº HF_HOMEã€‚Transformers/huggingface_hub ä¸‹è½½çš„æ¨¡å‹æƒé‡ã€é…ç½®æ–‡ä»¶éƒ½ä¼šæ”¾åœ¨è¿™é‡Œã€‚
3. æˆåŠŸåœ¨æ–°ç¯å¢ƒverlè¿è¡ŒåŸæœ‰é¡¹ç›®
4. TO-DOï¼š
   1. æ€»ç»“bashrcä¸­ä¿®æ”¹äº†ä»€ä¹ˆ
   2. è¿ç§»äº†å“ªäº›å†…å®¹ï¼Ÿé€šè¿‡mntä¸­æ–°å¢çš„å†…å®¹è½»æ¾å¾—çŸ¥

é—ç•™é—®é¢˜ï¼šå½“åªæœ‰<=2å¼ å¡æ—¶è¿è¡Œé¡¹ç›®ï¼Œæƒ³è¦æµ‹è¯•æ—¶ï¼Œä¼šå¡åœ¨ray actorå»ºç«‹éœ€è¦GPUä½†æ˜¯å¡ä¸å¤Ÿçš„æƒ…å†µã€‚

```
(autoscaler +9m40s) Tip: use `ray status` to view detailed cluster status. To disable these messages, set RAY_SCHEDULER_EVENTS=0.
(autoscaler +9m40s) Warning: The following resource request cannot be scheduled right now: {'CPU': 1.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
(autoscaler +10m15s) Warning: The following resource request cannot be scheduled right now: {'CPU': 1.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
...

```



### 2025.10.9

1. Qwen2.5-3Bå¯ä»¥åœ¨ä¸¤å—å¡ä¸Šè®­ç»ƒèµ·æ¥ã€‚

2. é¡¹ç›®ä¸­Rayè°ƒåº¦åŸç†ï¼šmain_ppo.pyæ˜¯é¡¹ç›®çš„ä¸»ç¨‹åºï¼Œå…¶ä¸­å·²ç»ç”¨Rayçš„ResourcePoolManagerç®¡ç†è®­ç»ƒå¡ã€‚

   - ä¼šæŠŠè¦è·‘çš„è§’è‰²ï¼ˆrolesï¼Œæœ¬é¡¹ç›®åŒ…æ‹¬ActorRollout / Critic / RefPolicy / RewardModelï¼‰æ˜ å°„åˆ°ä¸€ç»„Ray placement groupèµ„æºæ± ï¼Œèµ„æºæ± çš„å®¹é‡ä¸ºï¼š trainer.nnodes Ã— trainer.n_gpus_per_nodeï¼ŒæŒ‰ç…§ç»™æ¯ä¸ªroleé¢„ç•™ GPUï¼ˆä»¥åŠå¿…è¦çš„ CPU/å†…å­˜ï¼‰ï¼Œåœ¨æ‰€éœ€çš„èµ„æºä¸Šåˆ›å»ºå„ç±»workerã€‚æ¯ä¸ª worker åªâ€œçœ‹åˆ°â€è‡ªå·±é‚£å‡ å¼ å¡ï¼Œä»è€Œå®ç°å¡çš„ç‹¬å ä¸éš”ç¦»ã€‚

     - æŠŠâ€œè§’è‰²â€å˜æˆå¯è¿œç¨‹åˆ›å»ºçš„ Ray ç±»ï¼š

       ```python
       role_worker_mapping = {
                   Role.ActorRollout: ray.remote(actor_rollout_cls),
                   Role.Critic: ray.remote(CriticWorker),
               }
       ...
       role_worker_mapping[Role.RewardModel] = ray.remote(RewardModelWorker)
       ...
       role_worker_mapping[Role.RefPolicy] = ray.remote(ActorRolloutRefWorker)
       ```

     - å®šä¹‰æ¯ç±»è§’è‰²ç”¨å“ªä¸ªèµ„æºæ± ã€‚æœ¬é¡¹ç›®ä¸­æ‰€æœ‰è§’è‰²ä½¿ç”¨åŒæ ·çš„èµ„æºæ± ï¼ˆå› ä¸ºåªæœ‰ä¸€ä¸ªï¼‰ï¼š

       ```python
       global_pool_id = "global_pool"
       resource_pool_spec = {
         # æ¯ä¸ªå…ƒç´ æ˜¯ä¸€å°èŠ‚ç‚¹çš„ä¸€ä¸ª bundleï¼Œå€¼æ˜¯è¯¥èŠ‚ç‚¹ä¸ºè®­ç»ƒé¢„ç•™çš„ GPU æ•°
         # ä¾‹ï¼šnnodes=2, n_gpus_per_node=4 â†’ {"global_pool":[4,4]}
         global_pool_id: [config.trainer.n_gpus_per_node] * config.trainer.nnodes,
       }
       mapping = {
         Role.ActorRollout: global_pool_id,
         Role.Critic:       global_pool_id,
         # å¯é€‰
         Role.RewardModel:  global_pool_id,
         Role.RefPolicy:    global_pool_id,
       }
       ```

     - åˆ›å»ºç®¡ç†å™¨

       ```python
       resource_pool_manager = ResourcePoolManager(
         resource_pool_spec=resource_pool_spec,
         mapping=mapping
       )
       ```

     - æœ€åå¼€å§‹å å¡è®­ç»ƒï¼š

       ```python
       trainer = RayPPOTrainer(..., role_worker_mapping=..., resource_pool_manager=..., ...)
       # Initialize the workers of the trainer.
       trainer.init_workers()
       # Start the training process.
       trainer.fit()
       ```

   - testé˜¶æ®µå¯¼è‡´actor pendingæ— æ³•åˆ†é…åˆ°GPUçš„åŸå› ï¼š**å¡ä¸å¤Ÿæ‰€æœ‰actorsåˆ†çš„å°± Pending**ï¼š

     å¦‚æœ**å¿…é¡»åŒæ—¶ä½¿ç”¨å¡**çš„actorsæ‰€éœ€è¦çš„å¡çš„æ•°é‡çš„éœ€æ±‚ > æ‰€æœ‰å¯ç”¨çš„å¡æ•°ï¼ŒRay å°±æŠ¥ï¼š

     ```terminal
     cannot be scheduled ... {'CPU':1,'GPU':1}<--ä»¥ä¸Šæ˜¯æƒ³è¦å»ºç«‹æ–°çš„actoréœ€è¦çš„cpuå’Œgpuä¸ªæ•°ï¼Œä½†æ˜¯å…¶ä¸­æœ‰ä¸èƒ½æ»¡è¶³çš„
     ```

     å°±ä¼šæœ‰ actor PENDING_CREATIONã€‚è¦ä¹ˆå‡å°‘å¹¶è¡Œï¼Œè¦ä¹ˆæŠŠ trainer.n_gpus_per_node æˆ– CUDA_VISIBLE_DEVICES æ‰©åˆ°æ›´å¤šå¡ã€‚

   - è¿™æ¶‰åŠåˆ°äº†å…·ä½“çš„Ray actorå ç”¨GPUçš„åŸç†ï¼š

     - trainer.n_gpus_per_node=2 æ—¶ï¼Œ**placement group** ä¼šåœ¨è¯¥èŠ‚ç‚¹**é¢„ç•™ 2 å¼  GPU** ç»™è®­ç»ƒï¼ˆActorRollout/Critic/RefPolicy/RewardModel è¿™äº›â€œè®­ç»ƒè§’è‰²â€åœ¨æ•´ä¸ªè¿›ç¨‹ç”Ÿå‘½å‘¨æœŸä¸­ä¸€ç›´æŒæœ‰ PG çš„èµ„æºï¼Œä¸ä¼šä¸­é€”é‡Šæ”¾ï¼‰ã€‚
     - è¿™æ„å‘³ç€**é›†ç¾¤å±‚é¢**å·²ç»â€œè´¦é¢é”ä½â€äº†è¿™ä¸¤å¼ å¡ï¼Œå“ªæ€•æŸä¸ªè®­ç»ƒ worker æš‚æ—¶â€œé—²ç€â€ï¼Œå®ƒä¾ç„¶å ç€è¿™å¼ å¡ã€‚
     - å½“æµ‹è¯•é˜¶æ®µä½ çš„ **COMET** æƒ³è¦å¦èµ·ä¸€ä¸ª **ç‹¬ç«‹ Ray actorï¼ˆ@ray.remote(num_gpus=1)ï¼‰** æ—¶ï¼ŒRay çœ‹è´¦é¢æ²¡æœ‰â€œå‰©ä½™ GPUâ€ï¼Œäºæ˜¯ **è¿™ä¸ª COMET actor è¿›å…¥ Pending**ï¼Œå°±ä¼šä¸åœå‡ºç°ï¼š

     ```
     cannot be scheduled right now: {'CPU': 1.0, 'GPU': 1.0}
     ```

     - è€Œä¸”ä¼š**ä¸€ç›´ç­‰**ï¼Œé™¤éè®­ç»ƒé‡Šæ”¾ PGï¼ˆä¸ä¼šï¼‰æˆ–ä½ å‡å°‘è®­ç»ƒ PG çš„ GPU é¢„ç•™ã€æˆ–è€…æœºå™¨æœ‰æ›´å¤šå¯è§å¡ã€‚
     - åè¿‡æ¥ï¼Œtrainer.n_gpus_per_node=1 æ—¶ï¼ŒPG åªé” 1 å¼ å¡ï¼›å¦‚æœä½ æœºå™¨å¯è§ 2 å¼ å¡ï¼Œå°±è¿˜å‰© 1 å¼ è‡ªç”±å¡ï¼ŒCOMET çš„ num_gpus=1 actor å°±èƒ½æ’ä¸Š â†’ **ä¸ä¼š pending**ã€‚è¿™ä¹Ÿæ­£æ˜¯ä½ è§‚å¯Ÿåˆ°çš„ç°è±¡ã€‚

     - æ ¸å¿ƒç‚¹ï¼š**è®­ç»ƒçš„ PG é¢„ç•™ == é•¿æœŸå ä½**ï¼›ä½ å¦å¤–å†èµ·ä¸€ä¸ªâ€œç‹¬ç«‹å  GPU çš„ COMET actorâ€ï¼Œå°±å¿…é¡»æœ‰â€œPG ä¹‹å¤–çš„ç©ºé—²å¡â€ã€‚æ²¡æœ‰çš„è¯ï¼Œå°±ä¼šä¸€ç›´ Pendingã€‚

   - Ray actoræœ€ç»ˆä¿®æ”¹ï¼š**ä¸è¦æŠŠ COMET ä½œä¸ºç‹¬ç«‹ Ray GPU actor èµ·**ï¼›è€Œæ˜¯**è®­ç»ƒç”³è¯·çš„åŒä¸€ä¸ªactorå†…**ç›´æ¥åš COMET æ¨ç†ï¼ˆå…±äº«åŒä¸€ PG çš„ GPUï¼Œä¸æ–°å¢ num_gpus éœ€æ±‚ï¼‰ã€‚

3. é‡æ–°ç»™resultsæŒ‰ç…§GPUæ•°é‡å»ºç«‹checkpointç›®å½•ã€‚ä¸€èˆ¬æƒ…å†µä¸‹ä½¿ç”¨ä¸åŒæ•°é‡çš„GPUå¾—åˆ°çš„checkpointsä¸èƒ½é€šç”¨ã€‚

   ä»

   ```
   trainer.default_local_dir="${results_path}/qwen2.5_3b_r1-zero" \
   ```

   å˜æˆ

   ```
   trainer.default_local_dir="${results_path}/qwen2.5_3b_r1-zero/trainner_npus_${trainner_npus}" \
   ```

   



### 15/10 - 14/10/2025

1. Review the coding logic in reward and validation stages with different metrics.

2. The reason why COMET can only utilize cpu and cannot load on GPU:

   - In `main_ppo.py`, we have:

     ```python
     @ray.remote(num_cpus=1)  # please make sure main_task is not scheduled on head
     class TaskRunner:
     ....
     ```

     which decides that COMET computation is loaded in this TaskRunner. While this TaskRunner obtains the  Ray actor by setup a process on cpu `@ray.remote(num_cpus=1)`: TaskRunner itself NO GPU!!! So TaskRunner process cannot see and call GPU. Because Comet computation is in TaskRunner process, it cannot see GPU.

   - ä½†**è®­ç»ƒç”¨çš„é‚£äº› worker**ï¼ˆActor/Critic/Ref/RewardModel ç­‰ï¼‰éƒ½æ˜¯ **å•ç‹¬çš„ Ray actor**ï¼Œç”± ResourcePoolManager + placement group ç”³è¯·äº† GPUï¼ŒRay ä¼šç»™**æ¯ä¸ª worker è¿›ç¨‹**è®¾ç½®è‡ªå·±çš„ CUDA_VISIBLE_DEVICESï¼Œå› æ­¤å®ƒä»¬èƒ½åœ¨ GPU ä¸Šè·‘â€”â€”è¿™å’Œ TaskRunner æ˜¯å¦æœ‰ GPU å®Œå…¨æ— å…³ã€‚TaskRunner åªæ˜¯â€œæ€»æ§/è°ƒåº¦â€ï¼Œä¸å‚ä¸ç®—åŠ›ã€‚

   - å¦‚æœä¸æƒ³å†å•ç‹¬ä¸º COMET èµ·ä¸€ä¸ª GPU actorï¼ˆæ‹…å¿ƒé¢å¤–æŠ¢å¡æˆ– pendingï¼‰ã€‚é‚£å°±ç”¨==**æ–¹æ¡ˆ B**ï¼šæŠŠ COMET çš„åŠ è½½ä¸æ¨ç†åµŒå…¥**å·²æœ‰çš„ GPU worker**ï¼ˆä¾‹å¦‚ ActorRollout/RefPolicy/Critic ä»»æ„ä¸€ä¸ªè§’è‰²ï¼‰ï¼Œç”±è¯¥ worker åœ¨å®ƒè‡ªå·±çš„è¿›ç¨‹é‡Œï¼ˆå·²ç»æœ‰ GPU æš´éœ²çš„è¿›ç¨‹ï¼‰å®Œæˆ COMET æ¨ç†==ã€‚è¿™æ ·**æ²¡æœ‰æ–° actor**ï¼Œä¹Ÿä¸ä¼šå¤šç”³è¯· GPUï¼›COMET ä¸è®­ç»ƒå…±äº«è¿™å¼ ï¼ˆæˆ–å‡ å¼ ï¼‰å¡ã€‚

3. Realize the Ray actor assginment only on the trainning worker level, without exclusive gpu assginment for comet computation. --> test on both training (QE_MODE==off) and validation procedure

4. Add word level qe also in test stage. Just for a check

5. åˆ æ‰äº†åœ¨ Ray å¤šè¿›ç¨‹ç¯å¢ƒä¸­é‡æ–°åˆå§‹åŒ– CUDA ä¸Šä¸‹æ–‡ï¼ˆåŸå› è§gptï¼‰

6. Replace current "word-level QE" with xcomet and run through the code



### 16/10/2025

1. æŠŠæŒ‡æ ‡è®¡ç®—åŠ å…¥åˆ°å·²æœ‰çš„GPU worker

   1. è§£è¯»main_ppo.py

      - Assign all the specified available CUDA for training (TaskRunner).

        - Setting num_gpus on a actor reserves GPU(s) for that actor, making it unavailable to others while the actor is alive. It does not change the clusterâ€™s GPU capacity (specified in ray.init config); it just consumes from it.

      - When running TaskRunner, the config is defined through `/MT_Grpo_qe/verl/verl/trainer/config/ppo_trainer.yaml`, which has a basic config template maerged and overridden by CLI specified settings. We use Hydra (a multitask coordinater package) to coordinate them.

      - `ray.init(num_cpus=...)` vs `@ray.remote(num_cpus=1)` / `.options(num_gpus=1)`

        Theyâ€™re related but not overlapping:

        `ray.init(num_cpus=...)` sets the total CPU capacity Ray believes this node has (a pool/limit for scheduling). We donâ€™t pass `num_cpus` to `ray.init`, Ray auto-detects the machineâ€™s CPU count and uses that as cluster capacity.

        `@ray.remote(num_cpus=1)` / `.options(num_cpus=..., num_gpus=...)` specifies the resources an actor/task reserves when scheduled. Our actor has `@ray.remote(num_cpus=1)`, so the scheduler will reserve 1 logical CPU for it as long as at least 1 is available.

      - Now, trainer.profile_steps = null (which enables profiling instrumentation å›ºå®šæ­é…ï¼Œæ€§èƒ½åˆ†æ). We don't need it currently. We need it only if chase performance issues. It adds overhead å¼€é”€.

      - Add setting `ray.init: timeline_json_file: "ray_timeline.json"` . It is useful for diagnosing scheduling/perf issues across Ray workers.

      - **GPU assignment tips:**

        - If you do `TaskRunner.options(num_gpus=x)` (or `@ray.remote(num_gpus=x)`), that actor will hold x GPU for its lifetime. **If your training workers also need all GPUs, this will reduce whatâ€™s available for them.**
        - If your goal is to run COMET/XCOMET inside a training workerâ€™s existing GPU, **donâ€™t give the controller (`TaskRunner`) a GPU; instead, ensure the worker that computes rewards has `num_gpus=1`** and you run the metric on that same device (or coordinate device index explicitly). Otherwise you risk starving the vLLM/FSDP workers.

      - I didnâ€™t set `TaskRunner.options(num_gpus=1)`. Did TaskRunner â€œget all GPUsâ€ because of `ray.init(...)` with `CUDA_VISIBLE_DEVICES`?

        - **No.** GPUs are **not** reserved by `ray.init`. Ray only reserves GPUs when you specify `num_gpus` on a **task/actor** (`@ray.remote(num_gpus=...)` or `.options(num_gpus=...)`).
        - Without `num_gpus` on `TaskRunner`, it reserves **0 GPUs**. If your code inside TaskRunner calls CUDA directly, it *may* still **see** GPUs (via `CUDA_VISIBLE_DEVICES`) and try to use themâ€”but Ray wonâ€™t account for that usage â†’ **contention / OOM risk**.
        - Setting `CUDA_VISIBLE_DEVICES` globally in `runtime_env` only limits visibility; it does **not** reserve the devices. Other actors could still be scheduled to use the same GPUs unless they also reserve them.
        - **Bottom line:** You did **not** assign all GPUs to TaskRunner unless you explicitly set `num_gpus` on it.
        - **Ray can only protect you from Ray-scheduled contention.** If other students launch non-Ray jobs on the same machine, Ray canâ€™t stop them from taking â€œyourâ€ GPU and causing OOM. To truly reserve GPUs from *everyone*, you need **system/cluster-level isolation** (Slurm, Kubernetes quotas, or nvidia-smi exclusive mode by an admin). That said, hereâ€™s how to best reserve GPUs *within Ray* and keep your own job safe/consistent.
        - However, since we only instantialize one Ray task with only one actor, the single TaskRunner can *see* all GPUs because of CUDA_VISIBLE_DEVICES.
        - **TaskRunner è¦ä¸è¦ GPUï¼Ÿ**
          - **TaskRunner å°±æ˜¯ä¸€ä¸ª Ray actor**ï¼ˆä½ ä»£ç é¡¶éƒ¨æœ‰ @ray.remote(num_cpus=1)ï¼‰ï¼Œä½†å®ƒåªæ˜¯**è°ƒåº¦/æ§åˆ¶å™¨**ï¼šåˆ›å»ºæ•°æ®é›†ã€æ‹¼ worker å›¾ã€å¯åŠ¨è®­ç»ƒã€æ‹‰æ—¥å¿—ç­‰ã€‚
          - ç»™ **TaskRunner é… num_gpus å¹¶ä¸ä¼šè®© vLLM/FSDP çœŸæ­£ç”¨åˆ°è¿™äº› GPU**ï¼›ç›¸åï¼Œè¿™ä¼š**æŠŠ GPU èµ„æºé”åœ¨æ§åˆ¶å™¨è¿›ç¨‹**ä¸Šï¼Œå¯¼è‡´**çœŸæ­£å¹²æ´»çš„ GPU workerï¼ˆActorRollout/Critic/RefPolicyï¼‰æ‹¿ä¸åˆ°å¡**ã€‚
          - æˆ‘ä¹‹å‰æåˆ° TaskRunner.options(num_gpus=available_gpus) çš„è¯­å¢ƒæ˜¯â€œ**ç²—æš´å å¡**é˜²åˆ«äººæŠ¢â€ï¼Œä½†è¿™å’Œä½ å½“å‰ä½¿ç”¨ **RayPPOTrainer + FSDP/vLLM çš„å¤š GPU worker** æ¨¡å‹æ˜¯**å†²çªçš„**ï¼šä½ å ä½çš„å¡ï¼Œworker å°±ç”¨ä¸åˆ°äº†ã€‚å› æ­¤**åœ¨ä½ çš„æ¶æ„é‡Œä¸è¦ç»™ TaskRunner é… GPU**ã€‚
          - ç»“è®ºï¼š**ä¿æŒç°åœ¨çš„ @ray.remote(num_cpus=1) å°±å¥½**ï¼›GPU äº¤ç»™çœŸæ­£çš„ worker å»ç”³è¯·/ä½¿ç”¨ã€‚

      - If I set `ray.init: timeline_json_file: "ray_timeline.json"`, will it add overhead?

        - **Overhead:** minimal to moderate; Ray records events anyway and just dumps them when you call `ray.timeline`. Keeping it on occasionally is fine; for round-the-clock runs, leave it off unless diagnosing.



### 17/10/2025

å®ç°æ–¹æ¡ˆBã€‚ä½†æ˜¯é‡‡ç”¨çš„æ˜¯è‡ªè¡Œåˆ›å»ºRewardActorã€‚



### 18/10/2025 - 19/10/2025

å®ç°MetricRewardWorkerï¼Œç»“æ„ä¸RewardActorWorkerç›¸ä¼¼ï¼Œèå…¥verlæ¡†æ¶ï¼Œå°†å¥–åŠ±ä½œä¸ºå¹³è¡Œworkerã€‚



### 20/10/2025

è¿è¡Œå‘ç°åœ¨ç¬¬ä¸€è½®è®­ç»ƒä¸­å°±OOMã€‚åŸå› ï¼š

- XCOMET-XXL æœ¬ä½“éå¸¸å¤§ï¼ˆç£ç›˜ 40 GBï¼Œæ˜¾å­˜å ç”¨ä¹Ÿå¾ˆçŒ›ï¼‰ï¼Œè€Œä½ åŒæ—¶è¿˜åœ¨ GPU ä¸Šè·‘ï¼š

- Qwen2.5-3B çš„ Actor/Ref/VLLM KV-Cacheï¼›
- ä»¥åŠ COMETï¼ˆkiwi-xlï¼‰/XCOMETï¼ˆxxlï¼‰çš„æ¨ç†ã€‚

åœ¨åŒä¸€å°å¡ä¸Šâ€œæ’è½¦â€æ—¶ï¼Œå¾ˆå®¹æ˜“æŠŠ 80 GB A100 ç›´æ¥åƒçˆ†ï¼ˆä½ çš„å›æº¯é‡Œ OOM æ­£æ˜¯åœ¨ xcomet å‰å‘æ—¶å‘ç”Ÿçš„ï¼‰ã€‚

##### è§£å†³åŠæ³•ï¼š

é¦–å…ˆå°è¯•æ²¡æœ‰Xcometæ˜¯å¦èƒ½è¿è¡Œï¼š

- å‘ç°WORD_QE_MODE=offè¿˜æ˜¯ä¼šåŠ è½½xcometï¼šè¯´æ˜ä»£ç æœ‰é”™è¯¯ï¼Œæ²¡æœ‰æŒ‰ç…§WORD_QE_MODEçš„è¦æ±‚è°ƒæ•´åŠ è½½çš„æ¨¡å‹ï¼Œä¿®æ”¹ä»£ç ï¼š
  - å·²åˆ é™¤mix_weightså‚æ•°å’Œâ€œscoresâ€ key
    - å½“å‰ä»£ç å®ç°äº†æŠŠcometå’Œxcometæ”¾åˆ°GPUè®¡ç®—å¥–åŠ±å€¼ï¼Œä¹‹åä¼ å›comet_reward_batch_with_ray.pyå‡½æ•°ä¸formatå’Œbleuç»“åˆè®¡ç®—æœ€ç»ˆçš„æ¿€åŠ±å€¼
    - ä¿ç•™éªŒè¯é˜¶æ®µåˆ¤æ–­ã€‚å‘ç°`batch.py`è¢«æˆ‘æ›´æ”¹è¿‡ï¼Œé‚£ä¸ªæ—¶å€™æˆ‘æ˜¯ä¸ºäº†åŒºåˆ«è®­ç»ƒå’ŒéªŒè¯è¾“å‡ºçš„å¥–åŠ±æŒ‡æ ‡å€¼ä¸åŒè®¾ç½®çš„ã€‚ç»™BatchRewardManager.verify() added a config "compute_val_reward", which has a default value "None". 
      - ç›¸åº”çš„ï¼Œä¸ºäº†å¢åŠ â€œcompute_val_rewardâ€è°ƒç”¨å‚æ•°ï¼ŒåŒæ—¶æœŸæˆ‘è¿˜ä¿®æ”¹äº† `ray_trainer.py`,  è¿™ä¸¤ä¸ªæ–‡ä»¶éƒ½æ²¡æœ‰ä¿å­˜originalç‰ˆæœ¬

- ä»£ç ä¿®æ”¹æˆåŠŸï¼Œæ²¡æœ‰xcometè¿è¡Œ



### 21/10/2025

1. ä½†åˆå‘ç°cometæ¨¡å‹ä¸åœ¨CPUä¸ŠåŠ è½½ï¼š

   - åŸå› æ˜¯reward_fn.bind_rm_wg(...) æ²¡è¢«æ‰§è¡Œï¼ˆè§ä¸Šé¢çš„â€œå‡½æ•°å¯¹è±¡ vs æ¨¡å—å‡½æ•°â€é—®é¢˜ï¼‰ã€‚æŒ‰ç…§GPTæ‰€è¯´çš„æ›´æ”¹

   - å‘ç°_use_remote_worker() is not Noneï¼Œä½†æ˜¯

     ```
     out = _RM_WG.score(
                     src_mt_pairs=comet_data,
                     triplets=xcomet_data,
                     metrics=need_metrics
                 )
     ```

     **æŠ¥é”™AssertionError:**

     Solution: one line code above MetricRewardWorker.score():@register() should specify the dispatch mode as: @register(dispatch_mode=Dispatch.ONE_TO_ALL)

     **æŠ¥é”™TypeErrorï¼š**

     Solution: **ä½ è¿™ä»½ metric_worker.py é‡Œ score() æœ¬èº«â€œåœ¨å•ä¸ª worker ä¸Šâ€çš„è¿”å›å€¼ç¡®å®æ˜¯ Dict[str, List[float]]ã€‚** ä½†å› ä¸ºä½ ç»™å®ƒåŠ äº† @register(dispatch_mode=Dispatch.ONE_TO_ALL)ï¼Œå½“ä½ ä» **RayWorkerGroup ä¸Šè°ƒç”¨** rm_wg.score(...) æ—¶ï¼Œ**Ray ä¼šæŠŠæ¯ä¸ª DP rank çš„è¿”å›å€¼æ”¶é›†æˆä¸€ä¸ª list**ï¼Œæ‰€ä»¥**è°ƒç”¨æ–¹æ‹¿åˆ°çš„æ˜¯ List[Dict[str, List[float]]]**ï¼ˆæ¯ä¸ªå…ƒç´ å¯¹åº”ä¸€ä¸ª rankï¼‰ã€‚è¿™æ­£æ˜¯ä½ åœ¨ wandb é‡Œçœ‹åˆ° TypeError: list indices must be integers or slices, not str çš„åŸå› â€”â€”ä½ æŠŠè¿™ä»½ â€œlistâ€ å½“æˆ â€œdictâ€ å» out["comet"] äº†ã€‚

     - æ”¹è°ƒç”¨ä¾§ï¼ˆä¿æŒä½ å½“å‰ worker ä¸å˜ï¼‰

       åœ¨ comet_reward_batch_with_ray_debug.py é‡ŒåŠ ä¸€ä¸ªå°å·¥å…·_merge_remote_metric_out(out)ï¼Œå¹¶åœ¨è¿œç«¯åˆ†æ”¯ç”¨å®ƒ.

2. æˆåŠŸåœ¨GPUä¸ŠåŠ è½½æ¨¡å‹å¹¶è®¡ç®—comet

3. é¦–å…ˆå°è¯•è¿è¡Œoff mode 40è½®ï¼Œçœ‹çœ‹è®­ç»ƒæ•ˆæœï¼š

   - GPUåˆ©ç”¨ç‡å‘ˆç°**å‘¨æœŸæ€§çš„æ³¢å³°æ³¢è°·**ï¼š![image-20251021172254015](/Users/jingfanxin/Library/Application Support/typora-user-images/image-20251021172254015.png)

     #### åŸå› ï¼š

     éå¸¸å¥½çš„è§‚å¯Ÿ ğŸ‘ã€‚

     ä½ è¿™ä¸¤å¼  GPU åˆ©ç”¨ç‡æ›²çº¿éå¸¸å…¸å‹ï¼Œè¡¨ç°å‡ºä¸€ç§**â€œå‘¨æœŸæ€§æ³¢å³°æ³¢è°·â€**çš„ GPU è´Ÿè½½æ¨¡å¼ã€‚

     ä¸‹é¢æ˜¯ä½ çœ‹åˆ°è¿™ç§å‘¨æœŸæ€§â€œGPU åœå·¥â€çš„çœŸå®åŸå› åˆ†æ ğŸ‘‡

     

     **ğŸ§  ä¸€å¥è¯ç»“è®º**

     ä½ çš„ GPU åœ¨å‘¨æœŸæ€§åœ°â€œæš‚åœå·¥ä½œâ€ï¼Œä¸æ˜¯æ˜¾å¡å‡ºé—®é¢˜ï¼Œè€Œæ˜¯å› ä¸ºï¼š

     > **è®¡ç®—ï¼ˆforward/backwardï¼‰é˜¶æ®µåœ¨ GPU ä¸Šï¼Œéè®¡ç®—é˜¶æ®µï¼ˆreward è®¡ç®—ã€æ•°æ®æ•´ç†ã€Ray é€šä¿¡ã€CPU åŒæ­¥ã€æ—¥å¿—ã€advantage ä¼°è®¡ç­‰ï¼‰åœ¨ CPU ä¸Šæ‰§è¡Œã€‚**

     GPU åœ¨ç­‰å¾…è¿™äº› CPU æ“ä½œå®Œæˆæ—¶å°±ä¼šé—²ç½®ï¼Œäºæ˜¯å½¢æˆâ€œé«˜è´Ÿè½½ â†’ çŸ­æš‚åœ â†’ å†æ¬¡é«˜è´Ÿè½½â€çš„èŠ‚å¥ã€‚

     

     **ğŸ” ä»æ›²çº¿çœ‹é—®é¢˜**

   - **å·¦å›¾ï¼ˆGPU Utilizationï¼‰**ï¼š

     è§„å¾‹æ€§çš„é”¯é½¿å½¢æ³¢åŠ¨ï¼Œæ¯éš”å‡ åç§’æ‰åˆ° 0%ï¼Œéšååˆç¬é—´å›åˆ° 100%ã€‚

     â¤ è¡¨æ˜ GPU è®­ç»ƒæ˜¯ **é—´æ­‡æ€§çš„è®¡ç®—â€”ç­‰å¾…å¾ªç¯**ã€‚

   - **å³å›¾ï¼ˆMemory Allocatedï¼‰**ï¼š

     å†…å­˜ä¿æŒä¸­ç­‰åé«˜ï¼ˆçº¦ 40â€“80%ï¼‰ï¼Œæ²¡æœ‰é‡Šæ”¾ â†’ è¯´æ˜æ¨¡å‹å¸¸é©» GPUï¼Œåªæ˜¯è®¡ç®—æš‚æ—¶åœæ­¢ã€‚

     â¤ ä¸æ˜¯æ˜¾å­˜ç¢ç‰‡é—®é¢˜æˆ–é¢‘ç¹åŠ è½½æ¨¡å‹çš„é—®é¢˜ã€‚

     

     ##### **ğŸš¦ å‘¨æœŸæ€§æš‚åœçš„å¸¸è§åŸå› ï¼ˆæŒ‰å‡ºç°æ¦‚ç‡æ’åºï¼‰**

     **â‘  å¥–åŠ±è®¡ç®—é˜¶æ®µï¼ˆCOMET / BLEU / XCOMETï¼‰**

     æ¯ä¸ª rollout batch ç»“æŸåï¼Œä¼šï¼š

   - æŠŠç”Ÿæˆçš„ç¿»è¯‘ä» GPU æ‹·å› CPUï¼›

   - é€šè¿‡ Ray è°ƒç”¨è¿œç«¯ MetricRewardWorker.score()ï¼›

   - åŠ è½½å¹¶è¿è¡Œ COMET æ¨¡å‹ï¼ˆé€šå¸¸å•ç‹¬ GPU æˆ– CPU ä¸Šæ‰§è¡Œï¼‰ï¼›

   - æ±‡æ€»åå†å›ä¼  reward tensorã€‚

     â¡ï¸ åœ¨è¿™æœŸé—´ï¼Œ**ä¸»è®­ç»ƒ GPU åœ¨ç­‰ reward ç»“æœ**ï¼Œæ‰€ä»¥è´Ÿè½½æ‰åˆ° 0%ã€‚

     > ä½ å¯ä»¥åœ¨æ—¥å¿—ä¸­çœ‹åˆ°ï¼š

     ```
     Predicting DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:00<00:00]
     invalid items: 85 / 90
     ```

     > è¿™æ•´ä¸€åˆ†é’Ÿ GPU å‡ ä¹ä¸å¹²æ´»ï¼Œå°±æ˜¯ CPU/IO/Ray é˜¶æ®µã€‚

     

     **â‘¡ æ•°æ®åŠ è½½ï¼ˆDataloader + Tokenizationï¼‰**

     åœ¨æ¯ä¸ª batch ç»“æŸåï¼Œéœ€è¦ï¼š

   - è§£ç /ç¼–ç æ–‡æœ¬ï¼›

   - è½¬ tensorï¼›

   - æ‹¼ batchï¼›

   - åˆ†å‘åˆ°å„ rankï¼›

     è¿™äº›éƒ½åœ¨ CPU å®Œæˆï¼ŒGPU idleã€‚

     > è¡¨ç°ï¼šGPU åˆ©ç”¨ç‡æ‰ 0â€“10 ç§’ï¼ŒCPU å ç”¨çŸ­æ—¶é£™é«˜ã€‚

     

     **â‘¢ Ray actor é—´é€šä¿¡ / åŒæ­¥**

     ä½ ä½¿ç”¨ RayWorkerGroupï¼Œæ¯æ¬¡ _RM_WG.score(...) ä¼šï¼š

   - æŠŠ batch åˆ‡ç‰‡å‘é€ç»™è¿œç«¯ actorsï¼›

   - ç­‰å¾…è¿œç«¯è¿”å›ï¼›

   - èšåˆç»“æœã€‚

     å¦‚æœç½‘ç»œå»¶è¿Ÿæˆ–åºåˆ—åŒ–è€—æ—¶é«˜ï¼ŒGPU ä¹Ÿåœ¨â€œå¹²ç­‰â€ã€‚

     

     **â‘£ ä¼˜åŠ¿ä¼°è®¡ / å½’ä¸€åŒ–é˜¶æ®µ**

     åœ¨æ—¥å¿—ä¸­ä½ çœ‹åˆ°ï¼š

     ```
     ğŸ¯ GRPO ADVANTAGE ESTIMATION - Starting computation
     ğŸ“ˆ Advantages stats: min=..., mean=...
     ```

     è¿™æ®µæ˜¯åœ¨ CPU ä¸Šåš torch å¼ é‡å½’ä¸€åŒ–ç»Ÿè®¡çš„é˜¶æ®µã€‚

     è®¡ç®—é‡å°ã€I/O å¤š â†’ GPU idleã€‚

     

     ##### **ğŸ§© ä¸ºä»€ä¹ˆä¸æ˜¯â€œæ˜¾å­˜ä¸å¤Ÿâ€æˆ–â€œå¹¶è¡Œé—®é¢˜â€**

   - å¦‚æœæ˜¯æ˜¾å­˜é—®é¢˜ï¼šæ›²çº¿ä¼šå‡ºç°**çªé™ + é•¿æ—¶é—´ 0%**ï¼ˆç”šè‡³ crashï¼‰ã€‚

   - å¦‚æœæ˜¯åˆ†å¸ƒå¼æ­»é”ï¼šåˆ©ç”¨ç‡æ›²çº¿ä¼šä¸åŒæ­¥ï¼ˆè€Œä½ è¿™å‡ å¼ å‡ ä¹å®Œå…¨åŒæ­¥ï¼Œè¯´æ˜ FSDP åŒæ­¥è‰¯å¥½ï¼‰ã€‚

   - æ‰€ä»¥ä½ çš„ GPU åˆ†é…/å¹¶è¡Œæ²¡é—®é¢˜ï¼Œåªæ˜¯ pipeline æœ‰é GPU éƒ¨åˆ†æˆä¸ºç“¶é¢ˆã€‚

     

     ##### **ğŸš€ å¦‚ä½•è®© GPU åˆ©ç”¨ç‡æ›´å¹³æ»‘**

     **âœ… 1. å¼‚æ­¥å¥–åŠ±è®¡ç®—ï¼ˆAsync Reward)**

     ä¸è¦åœ¨ä¸»è®­ç»ƒ loop ç­‰ rewardã€‚

     å¯ä»¥ï¼š

   - æŠŠ MetricRewardWorker æ”¹ä¸ºå¼‚æ­¥ ray.remote(num_gpus=1)ï¼›

   - ä¸»è¿›ç¨‹æå‰ prepare ä¸‹ä¸€ä¸ª batchï¼›

   - æˆ–ä½¿ç”¨ queue å¼‚æ­¥è·å– reward ç»“æœã€‚

     ğŸ’¡ Ray å®˜æ–¹æ¨èçš„ pattern:

     ```
     futures = [remote_score.remote(batch_i) for batch_i in batches]
     results = ray.get(futures)
     ```

     ä¸» GPU å°±å¯ä»¥ç»§ç»­è·‘ä¸‹ä¸€æ‰¹ rolloutã€‚

     **âœ… 2. å¢åŠ  DataLoader worker æ•°**

     åœ¨ rollout é˜¶æ®µï¼š

     ```
     DataLoader(..., num_workers=8, pin_memory=True)
     ```

     è®© CPU èƒ½å¹¶è¡Œå‡†å¤‡æ•°æ®ã€‚

     **âœ… 3. ç¼©çŸ­ reward æ‰¹å¤„ç†å»¶è¿Ÿ**

     ä½ çš„ Predicting DataLoader 0: 1/1 [01:00<00:00] è¯´æ˜æ¯ä¸ª reward batch å¤ªå¤§ã€‚

     å¯å‡å° reward batch sizeï¼ˆä¾‹å¦‚ä» 90 é™åˆ° 32ï¼‰ï¼Œå‡å°‘æ¯æ¬¡ predict çš„ç­‰å¾…æ—¶é—´ã€‚

     **âœ… 4. æå‰ move tensors to GPU**

     æœ‰äº› reward å‡½æ•°ï¼ˆå°¤å…¶ BLEUã€COMETï¼‰ä¼šå…ˆåœ¨ CPU é¢„æµ‹åå† .to(device)ã€‚

     å¯æ”¹ä¸ºç›´æ¥åœ¨ GPU ä¸ŠåŠ è½½æ¨¡å‹å¹¶åœ¨ batch å†…è¿ç®—ï¼Œé¿å… device æ¥å›åˆ‡æ¢ã€‚

     

     **âœ… 5. è°ƒä½ logging / checkpoint é¢‘ç‡**

     æ¯æ¬¡ log åˆ° wandbã€ä¿å­˜ checkpoint éƒ½ä¼šé˜»å¡è®­ç»ƒå‡ ç§’ã€‚

     

     **ğŸ“Š å¦‚æœä½ æƒ³ç¡®è®¤æ¯ä¸ªé˜¶æ®µçš„æ—¶é—´å æ¯”**

     åœ¨è®­ç»ƒä¸»å¾ªç¯æ’å…¥ç®€å•çš„è®¡æ—¶å™¨ï¼š

     ```
     import time
     
     t0 = time.time()
     rewards = compute_reward(...)
     print(f"âš™ï¸ Reward compute time: {time.time()-t0:.2f}s")
     
     t1 = time.time()
     data = compute_advantage(...)
     print(f"ğŸ¯ Advantage compute time: {time.time()-t1:.2f}s")
     
     t2 = time.time()
     loss, stats = self.trainer.train_step(data)
     print(f"ğŸš€ Train step (GPU) time: {time.time()-t2:.2f}s")
     ```

     ä½ å°±èƒ½æ¸…æ¥šçœ‹åˆ°ï¼š

   - GPU éƒ¨åˆ†å ç”¨ï¼ˆtrain stepï¼‰

   - CPU é˜¶æ®µï¼ˆrewardã€advantageï¼‰

   - ç­‰å¾…æ—¶é—´ï¼ˆdata I/Oï¼‰

     

     **âœ… æ€»ç»“å›¾è§£**

     

     | **å‘¨æœŸé˜¶æ®µ**     | **ä¸»è¦è®¾å¤‡**  | **GPUçŠ¶æ€** | **åŸå› **            |
     | ---------------- | ------------- | ----------- | ------------------- |
     | forward/backward | GPU           | ğŸ”¥ é«˜è´Ÿè½½    | æ¨¡å‹è®­ç»ƒ            |
     | rewardè®¡ç®—       | CPU / è¿œç«¯GPU | ğŸ’¤ ç©ºé—²      | COMET/XCOMETè¯„åˆ†    |
     | advantageä¼°è®¡    | CPU           | ğŸ˜´ ç©ºé—²      | GRPOç»Ÿè®¡/æ ‡å‡†åŒ–     |
     | æ•°æ®å‡†å¤‡         | CPU           | ğŸ’¤ ç©ºé—²      | Dataloader/tokenize |
     | ä¸‹ä¸€ä¸ªbatchå¼€å§‹  | GPU           | ğŸ”¥ å†æ¬¡100%  | forwardå†èµ·         |

     ğŸ‘‰ **ä½ ç°åœ¨çœ‹åˆ°çš„ GPU â€œæ³¢åŠ¨â€å…¶å®æ˜¯æ•´ä¸ª RLHF/GRPO pipeline çš„è‡ªç„¶å‘¼å¸èŠ‚å¥ã€‚**

     åªè¦ GPU æ²¡é•¿æ—¶é—´é—²ç½®ï¼ˆ>50% æ—¶é—´åœ¨ idleï¼‰ï¼Œæ•´ä½“ pipeline å°±æ˜¯å¥åº·çš„ã€‚

     å¦‚æœä½ æƒ³è¿›ä¸€æ­¥å‹æ¦¨åˆ©ç”¨ç‡ï¼Œå¯ä»¥å°è¯• â€œå¼‚æ­¥å¥–åŠ± + å¤šçº¿ç¨‹æ•°æ®åŠ è½½ + å°æ‰¹é‡è¯„ä¼°â€ ä¸‰ä»¶äº‹ã€‚

4. **ä¸€æ¡ step ç”Ÿæˆä¸è®­ç»ƒæ‰€ç”¨çš„â€œæ ·æœ¬æ¡æ•°ï¼ˆåºåˆ—æ•°ï¼‰â€**ï¼š

   ```
   samples_per_step_total = data.train_batch_size  Ã—  actor_rollout_ref.rollout.n
   ```

5. æ›´æ–°ç­›é€‰æœ‰ç”¨çš„wadb projectsï¼Œå¹¶æ ¹æ®ä½¿ç”¨çš„å¥–åŠ±æŒ‡æ ‡ã€æ¨¡å¼å’Œä¸€è½®stepç”Ÿæˆä¸å¯»æ¥å‘¢æ‰€æœ‰çš„æ ·æœ¬æ¡æ•°å‘½åè¿™äº›projects

### 23/10/2025

1. äº†è§£wandbä¸Šé¢å„ç§å‚æ•°çš„æ„æ€

2. checkpointsä¿å­˜è·¯å¾„é‡æ–°å‘½åæ ¼å¼ï¼š

   ```
   trainer.default_local_dir="${results_path}/qwen2.5_3b_r1-zero/xcomet_${WORD_QE_MODE}/trainer_npus_${gpu_count}" \
   ```

   TODOï¼šæŠŠä¹‹å‰off 128çš„è·¯å¾„æ”¹åˆ°ï¼š

   ```
   trainer.default_local_dir="/mnt/data1/users/4xin/MT_Grpo_qe/results/qwen2.5_3b_r1-zero/xcomet_off/trainer_npus_4/"
   ```


### config

æ›´æ”¹GPUæ•°é‡ï¼š

```shell
# å½“GPUæ•°é‡=3æ—¶
actor_rollout_ref.rollout.n=6 \ # è¢«ä¸‰æ•´é™¤
actor_rollout_ref.actor.ppo_micro_batch_size_per_gpu=4 \ # æ–°å¢
actor_rollout_ref.rollout.log_prob_micro_batch_size=15 \
actor_rollout_ref.ref.log_prob_micro_batch_size=15\
data.train_batch_size=15 \
actor_rollout_ref.actor.ppo_mini_batch_size=15 \
trainer.n_gpus_per_node=3 \
rollout.n=6 \
# åˆ æ‰ï¼šä¸per gpuç»“å°¾çš„å‚æ•°å®é™…ä¸Šæ˜¯ä¸€ä¸ªä¸œè¥¿
actor_rollout_ref.actor.ppo_micro_batch_size=16  \
```



-------

### å¼€ä¼šçºªè¦

1. é¦–å…ˆæŠŠxcometçš„å„ç§å‚æ•°ä¿®æ”¹ä¸€ä¸‹ï¼š

   1. optmizerå æ˜¾å­˜è¿™ä»¶äº‹ï¼ˆæ‹ç…§äº†ï¼‰ï¼Œå¯ä»¥ä¿®æ”¹å‚æ•°ï¼Œé˜²æ­¢åˆ«äººè¶æ˜¾å­˜å ç”¨å°‘ï¼Œä¹ŸæŠŠå¡ç»™å äº†ã€‚

   2. éªŒè¯å­˜ç¿»è¯‘çš„å¥å­ï¼Œçœ‹ç¿»è¯‘æ•ˆæœï¼ˆæ‹ç…§äº†ï¼‰ï¼Œä¿®æ”¹å‚æ•°ï¼Œé¦–å…ˆè·‘ä¸‹è¯•è¯•

   3. å¦‚æœéœ€è¦æŠŠé¡¹ç›®å‘ç»™xintongè¿è¡Œï¼Œéœ€è¦ä¸Šä¼ åˆ°githubï¼Œå…¶ä¸­æ‰€æœ‰è·¯å¾„è¦ä¿®æ”¹æˆxintongå…«å—å¡çš„è·¯å¾„ç»“æ„ï¼ˆè¯¦æƒ…è§https://github.com/p1k0pan/zh_tox_lora README.mdæœ€ä¸‹æ–¹2025å¹´5æœˆ15æ—¥é‡Œckptsçš„è·¯å¾„ï¼‰

   4. ç°åœ¨sensecoreå¤–éƒ¨æœåŠ¡å™¨è¿˜èƒ½ç”¨ï¼Œå ç€å¡å‘¢ã€‚å°½å¿«æŠŠxcometæŒ‡æ ‡æ”¾åˆ°å¤–éƒ¨æœåŠ¡å™¨ä¸Šè¿è¡Œï¼Œåˆ«ç”¨XXLï¼Œç”¨XLæ¨¡å‹

2. çœ‹äº›paperæ‰¾æ‰¾æ€è·¯ï¼š

   1. éœ€è¦çœ‹ç”¨åˆ°çš„nonverifiableç¿»è¯‘ä»»åŠ¡å„ä¸ªä¸åŒçš„æ•°æ®é›†ï¼Œå…ˆçœ‹ï¼Œæ‰¾çµæ„Ÿã€‚ï¼ˆæ•°æ®é›†è¯´çš„æ˜¯https://arxiv.org/pdf/2510.06471ï¼‰

   2. çœ‹æœ‰æ²¡æœ‰ä»€ä¹ˆåˆ«çš„POå¯ä»¥å€Ÿé‰´ï¼ˆverlå®˜ç½‘algorithmæœ‰åˆ—ï¼Œå¦‚SPPO OPOï¼‰
   3. è¯»æ–‡ç« ï¼šINSTRUCTSCORE: Explainable Text Generation Evaluation with Fine-grained Feedbackï¼›Alleviating Distribution Shift in Synthetic Data for Machine Translation Quality Estimationï¼›MQM-APE: Toward High-Quality Error Annotation Predictors with Automatic Post-Editing in LLM Translation Evaluators etcï¼Œè¿˜æœ‰æ•°æ®é›†é‚£ç¯‡ï¼Œè¿˜å¯ä»¥çœ‹paperçš„referencesï¼Œçœ‹èƒ½ä¸èƒ½æ‰¾åˆ°ä»€ä¹ˆçµæ„Ÿ

3. æˆ‘ä»¬ç›®å‰çš„æ–¹å‘æ˜¯ï¼š
   1. åæ–¹å‘è¯æ˜GRPOå¯ç”¨æ€§è€ŒDAPO/é«˜ç†µæ•ˆæœæ²¡é‚£ä¹ˆå¥½
   2. ä»ä»€ä¹ˆè§’åº¦ï¼Œèƒ½å¤ŸæŠŠQEå’Œthinkingç»“åˆæˆä¸ºä¸€ä¸ªæ–¹æ³•
   3. é€‚åº”æ–°çš„æ•°æ®é›†ï¼Œæ¯”å¦‚æœ¯è¯­å¯†åº¦é«˜çš„ã€æ–‡å­¦æ•°æ®é›†ç­‰